[{"authors":null,"categories":null,"content":"I am currently pursuing research in intersection of vision, machine learning and robotics at Robotics Research Center, IIIT-Hyderabad under the guidance of Prof. Madhava Krishna. Our work is focused on fusing classical robotics with recent learning based techniques to solve the problem of Image based Visual Navigation in 6DoF. Recently, our work Deep Model Predictive Control For Visual Servoing was accepted at CoRL 2020, a premier conference on intersection of machine learning and robotics hosted by MIT. I am actively looking for research assistantships and applying to MS/PhD programs for Fall 2021.\nI am an open-source enthusiast and actively contribute to open-source. In summer 2018, I was lucky to be selected for Google Summer of Code'19 under the organization JdeRobot, Universidad Rey Juan Carlos to work on Visual States under the guidance of Prof. Jose Maria Canas Plaza. Moreover, I mentored GSoC'19 students and represented JdeRobot at Mentor Summit'19 held at Munich, Germany. I also contributed to CloudCV/EvalAI and mentored several students being a part of Google Code-In'19.\nAt college, I lead the software domain of Team SRMAUV and worked on solving real-world underwater challenges through Computer Vision and building the software stack with ROS. Our team participted in Robosub, SAUVC, NIOT-SAVe competing with teams worldwide, raised funds to build ZARNA, got sponsored by NVIDIA under GPU-grant program and presented our work URSIM at UT'19 IEEE OES Conference held at Kaosiung, Taiwan.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pushkalkatara.github.io/author/pushkal-katara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pushkal-katara/","section":"authors","summary":"I am currently pursuing research in intersection of vision, machine learning and robotics at Robotics Research Center, IIIT-Hyderabad under the guidance of Prof. Madhava Krishna. Our work is focused on fusing classical robotics with recent learning based techniques to solve the problem of Image based Visual Navigation in 6DoF.","tags":null,"title":"Pushkal Katara","type":"authors"},{"authors":["Pushkal Katara","YVS Harish","Harit Pandya","Abhinav Gupta","AadilMehdi Sanchawala","Gourav Kumar","Brojeshwar Bhowmick","K. Madhava Krishna"],"categories":[],"content":"","date":1602754150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602754150,"objectID":"e44b56354605b3754d65da245e156d31","permalink":"https://pushkalkatara.github.io/publication/corl/","publishdate":"2020-10-15T14:59:10+05:30","relpermalink":"/publication/corl/","section":"publication","summary":"The simplicity of the visual servoing approach makes it an attractive option for tasks dealing with vision-based control of robots in many real-world applications. However, attaining precise alignment for unseen environments pose a challenge to existing visual servoing approaches. While classical approaches assume a perfect world, the recent data-driven approaches face issues when generalizing to novel environments. In this paper, we aim to combine the best of both worlds. We present a deep model predictive visual servoing framework that can achieve precise alignment with optimal trajectories and can generalize to novel environments. Our framework consists of a deep network for optical flow predictions, which are used along with a predictive model to forecast future optical flow. For generating an optimal set of velocities we present a control network that can be trained on-the-fly without any supervision. Through extensive simulations on photo-realistic indoor settings of the popular Habitat framework, we show significant performance gain due to the proposed formulation vis-a-vis recent state of the art methods. Specifically, we show a faster convergence and an improved performance in trajectory length over recent approaches.","tags":["MPC","Visual Servoing","Machine Learning","CoRL-2020"],"title":"Deep Model Predictive Control For Visual Servoing","type":"publication"},{"authors":["Pushkal Katara"],"categories":[],"content":"","date":1594235432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594235432,"objectID":"1c35464da6396fc7c09e8c3d761d2279","permalink":"https://pushkalkatara.github.io/project/rl6dof/","publishdate":"2020-07-09T00:40:32+05:30","relpermalink":"/project/rl6dof/","section":"project","summary":"The project tries to extends the work Target-drive [Visual Navigation](https://www-cs.stanford.edu/groups/vision/pdf/zhu2017icra.pdf) using Deep Reinforcement Learning to support RL agent action space of 2/3/6DoF. Major changes includes adding support for training in [Habitat](https://github.com/facebookresearch/habitat-sim) Simulator and updates in network architecture to support high degree of freedom in agent action space. It aims to gain better generalization capabilities to new goals using a deep siamese actor-critic model with a policy as a function of goal and current visual state. The project contains extensive tests on Habitat-Gibson environments with multiple agent action space configurations including combinations of Surge, Sway, Heave, Roll, Pitch, Yaw.","tags":[],"title":"Visual Navigation using DeepRL in 2/3/6 DOFs","type":"project"},{"authors":["Pushkal Katara","Okan Asik","Jose Maria Canas Plaza"],"categories":[],"content":"","date":1573209349,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573209349,"objectID":"ddfae4707a184467a88ba33af3542b86","permalink":"https://pushkalkatara.github.io/project/visualstates/","publishdate":"2019-11-08T16:05:49+05:30","relpermalink":"/project/visualstates/","section":"project","summary":"Worked with organization JdeRobot on VisualStates, developing a tool for programming robot behaviors using Finite State Machines. It generates ROS node as output, in C++ or Python, which connects to the configured robot drivers at runtime. Implemented recursive automata import functionality and global-local namespace as new features to the tool. Wrote documentation, articles and worked in collaboration with mentors through weekly video sessions and daily emails. Developed simulation examples in Gazebo simulator using computer vision and sensor manipulation algorithms using Toyota Prius Car for autonomous navigation.","tags":["GSoC"],"title":"VisualStates - GSoC'18@JdeRobot","type":"project"},{"authors":["Pushkal Katara"],"categories":[],"content":"","date":1560853114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560853114,"objectID":"0d155d2302c68d566a8aef35c9bb6fb7","permalink":"https://pushkalkatara.github.io/project/darknet_ros/","publishdate":"2019-06-18T15:48:34+05:30","relpermalink":"/project/darknet_ros/","section":"project","summary":"Robotics Operating System package developed for object detection and tracking on ROS image topics. Object detection based on Yolo v3 and tracking based on Kalman Filter and Optical flow. Implemented multi-threading and thread synchronization techniques with fetch, detect, publish threads to capture, infer and publish results as ROS topics. Solved the issue of memory leak on Jetson TX1 on leggedrobotics repository. Deployed on Jetson TX1 with inference of 4 FPS. I built this as part of SRM Autonomous Underwater Vehicle stack.","tags":["Multithreading","ROS","Darknet","Yolov3"],"title":"darknet_ros_v3","type":"project"},{"authors":["Pushkal Katara","Mukul Khanna","Harshit Nagar","Annapurani Panaiyappan .K"],"categories":[],"content":"","date":1548842869,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548842869,"objectID":"f3f11af9dd1a229900a0685079aa61d1","permalink":"https://pushkalkatara.github.io/project/ursim/","publishdate":"2019-01-30T15:37:49+05:30","relpermalink":"/project/ursim/","section":"project","summary":"URSim is a 3D underwater simulation framework for Unmanned Underwater Vehicles (UUVs) developed using ROS and Unity3D. Worked in collaboration with team SRMAUV for AUVâ€™s simulation and presented the work at 2019 IEEE Symposium on Underwater Technology. 2019/04/16-19 @NSYSU, Kaohsiung, Taiwan.","tags":["SRMAUV","ROS","Unity"],"title":"URSim - Open Source Unmanned Underwater Vehicles Simulator","type":"project"}]